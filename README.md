# Classification-to-Comment-Toxicity
In this project, my goal is to detect the toxicity of comments on the Internet to make it a safe and clean place. I tried traditional ML models such as SVM and Decision trees, and then I tried Deep Learning models such as TextCNN and LSTM. Finally I compared them and tried to find out why one is outperforming the other.

In the recent years, unfriendly comments on the internet have been very common. The toxicity of comments is defined as “anything rude, disrespectful or otherwise likely to make someone leave a discussion”. Such toxicity will definitely ruin the internet environment. The purpose of this project is to detect toxicity of comments, which mainly extracted from online conversions. We first embedded each comment into a vector and tried some traditional machine learning models such as SVM and XGBoost to determine whether the comment was toxic and how toxic it was. Then we built a neural network called Long-Short Term Memory (LSTM) and compared these models. It turned out that LSTM outperformed traditional models by significant amount and shown excellent classification power. In addition, we built a user interface that took input sentences from users and determined its toxicity. Finally, We gave some representative words for toxicity and non-toxicity.
